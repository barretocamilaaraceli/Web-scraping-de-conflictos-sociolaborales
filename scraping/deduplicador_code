 deduplicador.py ‚Äî Limpieza y unificaci√≥n de noticias por similitud textual

import pandas as pd
from pathlib import Path
from rapidfuzz import fuzz, process

# =========================
# CONFIGURACI√ìN
# =========================
DATA_PATH = Path("data")
ARCHIVO_CLASIFICADO = DATA_PATH / "conflictos_clasificados.csv"
ARCHIVO_SALIDA = DATA_PATH / "conflictos_limpios.csv"

# =========================
# FUNCIONES
# =========================
def eliminar_duplicados_uid(df):
    """Elimina duplicados exactos por UID."""
    if "uid" not in df.columns:
        df["uid"] = df.apply(
            lambda x: hash((str(x.get("titulo", "")).lower().strip() + str(x.get("medio", "")).lower().strip())),
            axis=1,
        )
    antes = len(df)
    df = df.drop_duplicates(subset=["uid"])
    eliminados = antes - len(df)
    print(f"üßπ Eliminados {eliminados} duplicados exactos (por UID).")
    return df


def eliminar_duplicados_similares(df, threshold=90):
    """
    Elimina duplicados por similitud de t√≠tulos (fuzzy matching).
    Mantiene el m√°s informativo (m√°s largo o m√°s reciente).
    """
    df = df.copy()
    df["titulo_norm"] = df["titulo"].fillna("").str.lower().str.strip()
    df = df.sort_values(by=["fecha_relevamiento", "longitud_texto"], ascending=[False, False])

    vistos = set()
    indices_a_eliminar = set()
    titulos = df["titulo_norm"].tolist()

    for i, t1 in enumerate(titulos):
        if i in indices_a_eliminar or not t1:
            continue
        similares = process.extract(t1, titulos, scorer=fuzz.token_sort_ratio, limit=None)
        for j, (match, score, idx) in enumerate(similares):
            if idx == i or idx in indices_a_eliminar:
                continue
            if score >= threshold:
                indices_a_eliminar.add(idx)

    df_limpio = df.drop(df.index[list(indices_a_eliminar)]).drop(columns=["titulo_norm"])
    print(f"ü§ñ Eliminados {len(indices_a_eliminar)} duplicados por similitud textual ({len(df_limpio)} finales).")
    return df_limpio


def limpiar_dataset():
    """Carga, limpia y guarda el dataset sin duplicados."""
    if not ARCHIVO_CLASIFICADO.exists():
        print(f"‚ö†Ô∏è No se encontr√≥ {ARCHIVO_CLASIFICADO}")
        return

    df = pd.read_csv(ARCHIVO_CLASIFICADO)
    print(f"üìÑ Cargado: {len(df)} registros iniciales.")

    # Limpieza progresiva
    df = eliminar_duplicados_uid(df)
    df = eliminar_duplicados_similares(df, threshold=90)

    # Guardar resultado
    df.to_csv(ARCHIVO_SALIDA, index=False, encoding="utf-8-sig")
    print(f"\n‚úÖ Dataset limpio guardado: {ARCHIVO_SALIDA} ({len(df)} registros √∫nicos).")


# =========================
# MAIN
# =========================
if __name__ == "__main__":
    print("=== Limpieza avanzada de duplicados (por UID + similitud) ===\n")
    limpiar_dataset()
    print("\nüèÅ Proceso completado con √©xito.") 
    
